{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据的加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pickle\n",
    "sys.path.append(\"C:/Users/Administrator/ud120-projects-master/tools/\")\n",
    "sys.path.append(\"C:/Users/Administrator/ud120-projects-master/choose_your_own\")\n",
    "sys.path.append(\"C:/Users/Administrator/ud120-projects-master/final_project/\")\n",
    "\n",
    "import os\n",
    "os.chdir('C:/Users/Administrator/ud120-projects-master/final_project')\n",
    "\n",
    "### Load the dictionary containing the dataset\n",
    "with open(\"final_project_dataset.pkl\", \"r\") as data_file:\n",
    "    data_dict = pickle.load(data_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据预览"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bonus</th>\n",
       "      <th>deferral_payments</th>\n",
       "      <th>deferred_income</th>\n",
       "      <th>director_fees</th>\n",
       "      <th>email_address</th>\n",
       "      <th>exercised_stock_options</th>\n",
       "      <th>expenses</th>\n",
       "      <th>fraction_from_poi</th>\n",
       "      <th>fraction_to_poi</th>\n",
       "      <th>from_messages</th>\n",
       "      <th>...</th>\n",
       "      <th>long_term_incentive</th>\n",
       "      <th>other</th>\n",
       "      <th>poi</th>\n",
       "      <th>restricted_stock</th>\n",
       "      <th>restricted_stock_deferred</th>\n",
       "      <th>salary</th>\n",
       "      <th>shared_receipt_with_poi</th>\n",
       "      <th>to_messages</th>\n",
       "      <th>total_payments</th>\n",
       "      <th>total_stock_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ALLEN PHILLIP K</th>\n",
       "      <td>4175000</td>\n",
       "      <td>2869717</td>\n",
       "      <td>-3081055</td>\n",
       "      <td>NaN</td>\n",
       "      <td>phillip.allen@enron.com</td>\n",
       "      <td>1729541</td>\n",
       "      <td>13868</td>\n",
       "      <td>0.0161957</td>\n",
       "      <td>0.0296128</td>\n",
       "      <td>2195</td>\n",
       "      <td>...</td>\n",
       "      <td>304805</td>\n",
       "      <td>152</td>\n",
       "      <td>False</td>\n",
       "      <td>126027</td>\n",
       "      <td>-126027</td>\n",
       "      <td>201955</td>\n",
       "      <td>1407</td>\n",
       "      <td>2902</td>\n",
       "      <td>4484442</td>\n",
       "      <td>1729541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BADUM JAMES P</th>\n",
       "      <td>NaN</td>\n",
       "      <td>178980</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>257817</td>\n",
       "      <td>3486</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>182466</td>\n",
       "      <td>257817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BANNANTINE JAMES M</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-5104</td>\n",
       "      <td>NaN</td>\n",
       "      <td>james.bannantine@enron.com</td>\n",
       "      <td>4046157</td>\n",
       "      <td>56301</td>\n",
       "      <td>0.0689046</td>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>864523</td>\n",
       "      <td>False</td>\n",
       "      <td>1757552</td>\n",
       "      <td>-560222</td>\n",
       "      <td>477</td>\n",
       "      <td>465</td>\n",
       "      <td>566</td>\n",
       "      <td>916197</td>\n",
       "      <td>5243487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BAXTER JOHN C</th>\n",
       "      <td>1200000</td>\n",
       "      <td>1295738</td>\n",
       "      <td>-1386055</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6680544</td>\n",
       "      <td>11200</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1586055</td>\n",
       "      <td>2660303</td>\n",
       "      <td>False</td>\n",
       "      <td>3942714</td>\n",
       "      <td>NaN</td>\n",
       "      <td>267102</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5634343</td>\n",
       "      <td>10623258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BAY FRANKLIN R</th>\n",
       "      <td>400000</td>\n",
       "      <td>260455</td>\n",
       "      <td>-201641</td>\n",
       "      <td>NaN</td>\n",
       "      <td>frank.bay@enron.com</td>\n",
       "      <td>NaN</td>\n",
       "      <td>129142</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>69</td>\n",
       "      <td>False</td>\n",
       "      <td>145796</td>\n",
       "      <td>-82782</td>\n",
       "      <td>239671</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>827696</td>\n",
       "      <td>63014</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      bonus deferral_payments deferred_income director_fees  \\\n",
       "ALLEN PHILLIP K     4175000           2869717        -3081055           NaN   \n",
       "BADUM JAMES P           NaN            178980             NaN           NaN   \n",
       "BANNANTINE JAMES M      NaN               NaN           -5104           NaN   \n",
       "BAXTER JOHN C       1200000           1295738        -1386055           NaN   \n",
       "BAY FRANKLIN R       400000            260455         -201641           NaN   \n",
       "\n",
       "                                 email_address exercised_stock_options  \\\n",
       "ALLEN PHILLIP K        phillip.allen@enron.com                 1729541   \n",
       "BADUM JAMES P                              NaN                  257817   \n",
       "BANNANTINE JAMES M  james.bannantine@enron.com                 4046157   \n",
       "BAXTER JOHN C                              NaN                 6680544   \n",
       "BAY FRANKLIN R             frank.bay@enron.com                     NaN   \n",
       "\n",
       "                   expenses fraction_from_poi fraction_to_poi from_messages  \\\n",
       "ALLEN PHILLIP K       13868         0.0161957       0.0296128          2195   \n",
       "BADUM JAMES P          3486                 0               0           NaN   \n",
       "BANNANTINE JAMES M    56301         0.0689046               0            29   \n",
       "BAXTER JOHN C         11200                 0               0           NaN   \n",
       "BAY FRANKLIN R       129142                 0               0           NaN   \n",
       "\n",
       "                          ...        long_term_incentive    other    poi  \\\n",
       "ALLEN PHILLIP K           ...                     304805      152  False   \n",
       "BADUM JAMES P             ...                        NaN      NaN  False   \n",
       "BANNANTINE JAMES M        ...                        NaN   864523  False   \n",
       "BAXTER JOHN C             ...                    1586055  2660303  False   \n",
       "BAY FRANKLIN R            ...                        NaN       69  False   \n",
       "\n",
       "                   restricted_stock restricted_stock_deferred  salary  \\\n",
       "ALLEN PHILLIP K              126027                   -126027  201955   \n",
       "BADUM JAMES P                   NaN                       NaN     NaN   \n",
       "BANNANTINE JAMES M          1757552                   -560222     477   \n",
       "BAXTER JOHN C               3942714                       NaN  267102   \n",
       "BAY FRANKLIN R               145796                    -82782  239671   \n",
       "\n",
       "                   shared_receipt_with_poi to_messages total_payments  \\\n",
       "ALLEN PHILLIP K                       1407        2902        4484442   \n",
       "BADUM JAMES P                          NaN         NaN         182466   \n",
       "BANNANTINE JAMES M                     465         566         916197   \n",
       "BAXTER JOHN C                          NaN         NaN        5634343   \n",
       "BAY FRANKLIN R                         NaN         NaN         827696   \n",
       "\n",
       "                   total_stock_value  \n",
       "ALLEN PHILLIP K              1729541  \n",
       "BADUM JAMES P                 257817  \n",
       "BANNANTINE JAMES M           5243487  \n",
       "BAXTER JOHN C               10623258  \n",
       "BAY FRANKLIN R                 63014  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(data_dict)\n",
    "df = df.T   #数据框转置\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据探索"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of persons :145\n",
      "Number of features : 23\n",
      "Number of data points: 3335\n",
      "Number of POI: 18\n",
      "Number of not POI: 127\n"
     ]
    }
   ],
   "source": [
    "print 'Number of persons :{0}'.format(len(df))\n",
    "print 'Number of features : {0}'.format(len(data_dict.values()[0]))\n",
    "print 'Number of data points:', len(df.index) * len(df.columns)\n",
    "print 'Number of POI:', len(df['poi'][df['poi'] == True])\n",
    "print 'Number of not POI:', len(df['poi'][df['poi'] == False])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据清洗"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       bonus deferral_payments deferred_income director_fees email_address  \\\n",
      "count    145               145             145           145           145   \n",
      "unique    41                39              44            17           112   \n",
      "top      NaN               NaN             NaN           NaN           NaN   \n",
      "freq      64               107              97           129            34   \n",
      "\n",
      "       exercised_stock_options expenses  fraction_from_poi  fraction_to_poi  \\\n",
      "count                      145      145              145.0            145.0   \n",
      "unique                     101       94               75.0             62.0   \n",
      "top                        NaN      NaN                0.0              0.0   \n",
      "freq                        44       51               71.0             79.0   \n",
      "\n",
      "       from_messages        ...        long_term_incentive other    poi  \\\n",
      "count            145        ...                        145   145    145   \n",
      "unique            65        ...                         52    92      2   \n",
      "top              NaN        ...                        NaN   NaN  False   \n",
      "freq              59        ...                         80    53    127   \n",
      "\n",
      "       restricted_stock restricted_stock_deferred salary  \\\n",
      "count               145                       145    145   \n",
      "unique               97                        18     94   \n",
      "top                 NaN                       NaN    NaN   \n",
      "freq                 36                       128     51   \n",
      "\n",
      "       shared_receipt_with_poi to_messages total_payments total_stock_value  \n",
      "count                      145         145            145               145  \n",
      "unique                      84          87            125               124  \n",
      "top                        NaN         NaN            NaN               NaN  \n",
      "freq                        59          59             21                20  \n",
      "\n",
      "[4 rows x 23 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看出除了poi特征的top是False，其他特征的top都是NaN,特征的缺失值比较多。  \n",
    "在Python中用NaN来标记缺失值，值为NaN的数据均不参与如求和、计数类的运算，后面只是对NaN进行描述性统计，对部分特征的NaN进行处理。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 统计数据集中 NaN 的情况\n",
    "import numpy as np\n",
    "df.replace('NaN', np.nan, inplace = True)\n",
    "features_with_nan = df.isnull().sum().sort_values(ascending=False)\n",
    "person_with_nan = df.isnull().sum(axis = 1).sort_values(ascending=False)  #按clumns lables 排序"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 10 features_with_nan\n",
      "loan_advances                142\n",
      "director_fees                129\n",
      "restricted_stock_deferred    128\n",
      "deferral_payments            107\n",
      "deferred_income               97\n",
      "long_term_incentive           80\n",
      "bonus                         64\n",
      "from_messages                 59\n",
      "from_poi_to_this_person       59\n",
      "from_this_person_to_poi       59\n",
      "dtype: int64\n",
      "\n",
      "Top 10 person_with_nan\n",
      "LOCKHART EUGENE E                20\n",
      "WROBEL BRUCE                     18\n",
      "WHALEY DAVID A                   18\n",
      "GRAMM WENDY L                    18\n",
      "THE TRAVEL AGENCY IN THE PARK    18\n",
      "GILLIS JOHN                      17\n",
      "WODRASKA JOHN                    17\n",
      "SAVAGE FRANK                     17\n",
      "SCRIMSHAW MATTHEW                17\n",
      "CLINE KENNETH W                  17\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# 打印出缺失值最多的前 10位\n",
    "print '\\nTop 10 features_with_nan\\n', features_with_nan[:10]\n",
    "print '\\nTop 10 person_with_nan\\n', person_with_nan[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**缺失值** ： \n",
    "- 可以看出loan_advances 缺失值有142，只有4条记录。该特征不具有代表性，将该特征删除。  \n",
    "- person里面的缺失值较多，其中LOCKHART EUGENE E，只有一条数据即poi，其他person中缺失值也较多。\n",
    "\n",
    "**异常值**：\n",
    "\n",
    "- 把person中THE TRAVEL AGENCY IN THE PARK 删除，明显不是人名\n",
    "- 删除TOTAL,为财务数据的总和\n",
    "\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "labels ['TOTAL'] not contained in axis",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-39906e79c2c7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# 特征的处理\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'TOTAL'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'LOCKHART EUGENE E'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'THE TRAVEL AGENCY IN THE PARK'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'email_address'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m     \u001b[1;31m#email_address 不具有唯一性\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda2\\lib\\site-packages\\pandas\\core\\generic.pyc\u001b[0m in \u001b[0;36mdrop\u001b[1;34m(self, labels, axis, level, inplace, errors)\u001b[0m\n\u001b[0;32m   2159\u001b[0m                 \u001b[0mnew_axis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2160\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2161\u001b[1;33m                 \u001b[0mnew_axis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2162\u001b[0m             \u001b[0mdropped\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0maxis_name\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mnew_axis\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2163\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda2\\lib\\site-packages\\pandas\\core\\indexes\\base.pyc\u001b[0m in \u001b[0;36mdrop\u001b[1;34m(self, labels, errors)\u001b[0m\n\u001b[0;32m   3622\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0merrors\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m'ignore'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3623\u001b[0m                 raise ValueError('labels %s not contained in axis' %\n\u001b[1;32m-> 3624\u001b[1;33m                                  labels[mask])\n\u001b[0m\u001b[0;32m   3625\u001b[0m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m~\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3626\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: labels ['TOTAL'] not contained in axis"
     ]
    }
   ],
   "source": [
    "# 特征的处理\n",
    "df = df.drop(['TOTAL', 'LOCKHART EUGENE E', 'THE TRAVEL AGENCY IN THE PARK'], axis = 0)\n",
    "df = df.drop(['email_address'], axis = 1)     #email_address 不具有唯一性"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 特征工程  \n",
    " **[label]**： \n",
    "- 1. poi 是否是嫌疑人 (True=POI, Flase=Non-POI)  \n",
    "**[邮件]**：\n",
    "- 2. email_address          : 电子邮件地址\n",
    "- 3. to_messages            : 发送的电子邮件数量\n",
    "- 4. from_messages          : 收到的电子邮件数量\n",
    "- 5. from_poi_to_this_person: 收到的来自 POI 的邮件数量\n",
    "- 6. from_this_person_to_poi: 发送给 POI 的邮件数量\n",
    "- 7. shared_receipt_with_poi: 收到的来自 POI 的抄送邮件  \n",
    "**[财务]**：\n",
    "- 8. salary                    : 工资\n",
    "- 9. bonus                     : 奖金\n",
    "- 10. long_term_incentive      : 长期激励薪酬  \n",
    "- 11. deferred_income          : 延递收入  \n",
    "- 12. deferral_payments        : 延期薪酬  \n",
    "- 13. loan_advances            : 预支贷款  \n",
    "- 14. other  \n",
    "- 15. expenses                 : 开支  \n",
    "- 16. director_fees            : 董事费  \n",
    "- 17. total_payments           : 总薪酬和奖金  \n",
    "- 18. exercised_stock_options  : 已行使过的股票期权\n",
    "- 19. restricted_stock         : 受限股票\n",
    "- 20. restricted_stock_deferred: 延期的受限股票\n",
    "- 21. total_stock_value        : 总股票价值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 建立新特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['fraction_from_poi'] = df['from_poi_to_this_person'] / df['to_messages']\n",
    "df['fraction_to_poi'] = df['from_this_person_to_poi'] / df['from_messages']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "显示新增了2个字段，共22个字段"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from feature_format import featureFormat, targetFeatureSplit\n",
    "from tester import dump_classifier_and_data\n",
    "from tester import test_classifier\n",
    "\n",
    "### Task 1: Select what features you'll use.\n",
    "### features_list is a list of strings, each of which is a feature name.\n",
    "### The first feature must be \"poi\".\n",
    "features_list = ['poi','salary','bonus','long_term_incentive','deferred_income',\n",
    "                 'deferral_payments','total_payments','exercised_stock_options',\n",
    "                 'restricted_stock','restricted_stock_deferred','total_stock_value'] # You will need to use more features\n",
    "\n",
    "### Load the dictionary containing the dataset\n",
    "with open(\"final_project_dataset.pkl\", \"r\") as data_file:\n",
    "    data_dict = pickle.load(data_file)\n",
    "\n",
    "### Task 2: Remove outliers\n",
    "### Task 3: Create new feature(s)\n",
    "\n",
    "### Store to my_dataset for easy export below.\n",
    "data_dict.pop(\"TOTAL\",0)\n",
    "\n",
    "for i in data_dict:\n",
    "    person = data_dict[i]\n",
    "    if (all([person['from_poi_to_this_person'] != 'NaN',\n",
    "             person['from_this_person_to_poi'] != 'NaN',\n",
    "             person['to_messages'] != 'NaN',\n",
    "             person['from_messages'] != 'NaN'])):\n",
    "        fraction_from_poi = float(person[\"from_poi_to_this_person\"]) / float(person[\"to_messages\"])\n",
    "        person[\"fraction_from_poi\"] = fraction_from_poi\n",
    "        fraction_to_poi = float(person[\"from_this_person_to_poi\"]) / float(person[\"from_messages\"])\n",
    "        person[\"fraction_to_poi\"] = fraction_to_poi\n",
    "    else:\n",
    "        person[\"fraction_from_poi\"] = person[\"fraction_to_poi\"] = 0\n",
    "\n",
    "my_features_list = ['poi','salary','bonus','long_term_incentive','deferred_income',\n",
    "                    'deferral_payments','total_payments','exercised_stock_options',\n",
    "                    'restricted_stock','restricted_stock_deferred','total_stock_value',\n",
    "                    'fraction_from_poi','fraction_to_poi']\n",
    "\n",
    "my_dataset = data_dict\n",
    "\n",
    "### Extract features and labels from dataset for local testing\n",
    "data = featureFormat(my_dataset, my_features_list, sort_keys = True)\n",
    "labels, features = targetFeatureSplit(data)\n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "features_train, features_test, labels_train, labels_test = \\\n",
    "    train_test_split(features, labels, test_size=0.3, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 特征选择"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 18.57570327,  21.06000171,  10.07245453,  11.59554766,\n",
       "         0.21705893,   8.86672154,  25.09754153,   9.34670079,\n",
       "         0.06498431,  24.46765405,   3.21076192,  16.64170707])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "selector = SelectKBest(k='all').fit(features,labels)\n",
    "selector.transform(features) \n",
    "selector.scores_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "选择得分10分以上的，选择k=7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  7.90181112e-01,   1.64856450e-01,   1.99246216e-02,\n",
       "         1.01101602e-02,   7.32937146e-03,   4.28014808e-03,\n",
       "         2.07746544e-03,   1.06996431e-03,   1.12390967e-04,\n",
       "         5.83165845e-05,   2.06153327e-16,   6.38784948e-18])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca=PCA(n_components=None)\n",
    "pca.fit_transform(features)\n",
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "创建了2个新的特征，fraction_from_poi(from_poi_to_this_person/to_messages)和fraction_to_poi(from_this_person_to_poi/from_messages),  \n",
    "这两个新的特征反映的是与POI邮件来往的频率。加进my_features_list总共有13个特征，用PCA进行降维，PCA(n_components=3)时，pca.explained_variance_ratio_为[ 0.79018111  0.16485645  0.01992462]，最后一个能够对该数据集解释的概率为0.0199，  \n",
    "故考虑PCA(n_components=2)，此时pca.explained_variance_ratio为[ 0.79018111  0.16485645]，此时特征为2维特征。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy_lr: 0.909\n",
      "Pipeline(memory=None,\n",
      "     steps=[('sc', StandardScaler(copy=True, with_mean=True, with_std=True)), ('pca', PCA(copy=True, iterated_power='auto', n_components=2, random_state=None,\n",
      "  svd_solver='auto', tol=0.0, whiten=False)), ('clf', LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False))])\n",
      "\tAccuracy: 0.87327\tPrecision: 0.58347\tRecall: 0.17300\tF1: 0.26687\tF2: 0.20133\n",
      "\tTotal predictions: 15000\tTrue positives:  346\tFalse positives:  247\tFalse negatives: 1654\tTrue negatives: 12753\n",
      "\n",
      "Test accuracy_nb: 0.864\n",
      "Pipeline(memory=None,\n",
      "     steps=[('sc', StandardScaler(copy=True, with_mean=True, with_std=True)), ('pca', PCA(copy=True, iterated_power='auto', n_components=2, random_state=None,\n",
      "  svd_solver='auto', tol=0.0, whiten=False)), ('clf', GaussianNB(priors=None))])\n",
      "\tAccuracy: 0.85613\tPrecision: 0.44684\tRecall: 0.33200\tF1: 0.38095\tF2: 0.34999\n",
      "\tTotal predictions: 15000\tTrue positives:  664\tFalse positives:  822\tFalse negatives: 1336\tTrue negatives: 12178\n",
      "\n",
      "Test accuracy_tr: 0.841\n",
      "Pipeline(memory=None,\n",
      "     steps=[('sc', StandardScaler(copy=True, with_mean=True, with_std=True)), ('pca', PCA(copy=True, iterated_power='auto', n_components=2, random_state=None,\n",
      "  svd_solver='auto', tol=0.0, whiten=False)), ('clf', DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_...      min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best'))])\n",
      "\tAccuracy: 0.81207\tPrecision: 0.25841\tRecall: 0.21900\tF1: 0.23708\tF2: 0.22589\n",
      "\tTotal predictions: 15000\tTrue positives:  438\tFalse positives: 1257\tFalse negatives: 1562\tTrue negatives: 11743\n",
      "\n",
      "Test accuracy_rf: 0.864\n",
      "Pipeline(memory=None,\n",
      "     steps=[('pca', PCA(copy=True, iterated_power='auto', n_components=2, random_state=None,\n",
      "  svd_solver='auto', tol=0.0, whiten=False)), ('clf', RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            ...n_jobs=1,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False))])\n",
      "\tAccuracy: 0.84053\tPrecision: 0.27626\tRecall: 0.12100\tF1: 0.16829\tF2: 0.13632\n",
      "\tTotal predictions: 15000\tTrue positives:  242\tFalse positives:  634\tFalse negatives: 1758\tTrue negatives: 12366\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Task 4: Try a varity of classifiers\n",
    "### Please name your classifier clf for easy export below.\n",
    "### Note that if you want to do PCA or other multi-stage operations,\n",
    "### you'll need to use Pipelines. For more info:\n",
    "### http://scikit-learn.org/stable/modules/pipeline.html\n",
    "\n",
    "# Provided to give you a starting point. Try a variety of classifiers.\n",
    "\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import KernelPCA\n",
    "\n",
    "# Provided to give you a starting point. Try a variety of classifiers.\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "#LogisticRegression\n",
    "pipe_lr = Pipeline([('sc', StandardScaler()),\n",
    "                    ('pca', PCA(n_components=2)),\n",
    "                    ('clf', LogisticRegression())])   #random_state=1\n",
    "pipe_lr.fit(features_train, labels_train)\n",
    "print('Test accuracy_lr: %.3f' % pipe_lr.score(features_test, labels_test))\n",
    "test_classifier(pipe_lr, my_dataset, my_features_list)\n",
    "\n",
    "#GaussianNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "pipe_nb = Pipeline([('sc', StandardScaler()),\n",
    "                    ('pca', PCA(n_components=2)),\n",
    "                    ('clf', GaussianNB()) ])\n",
    "pipe_nb.fit(features_train, labels_train)\n",
    "print('Test accuracy_nb: %.3f' % pipe_nb.score(features_test, labels_test))\n",
    "test_classifier(pipe_nb, my_dataset, my_features_list)\n",
    "\n",
    "#DecisionTreeClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "tree = DecisionTreeClassifier()\n",
    "pipe_tr = Pipeline([ ('sc', StandardScaler()),\n",
    "                    ('pca', PCA(n_components=2)),\n",
    "                    ('clf', tree)])\n",
    "pipe_tr.fit(features_train, labels_train)\n",
    "print('Test accuracy_tr: %.3f' % pipe_tr.score(features_test, labels_test))\n",
    "test_classifier(pipe_tr, my_dataset, my_features_list)\n",
    "\n",
    "#RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = RandomForestClassifier()   #max_depth=2, random_state=0\n",
    "pipe_rf = Pipeline([('pca', PCA(n_components=2)),\n",
    "                    ('clf', clf)])\n",
    "pipe_rf.fit(features_train, labels_train)\n",
    "print('Test accuracy_rf: %.3f' % pipe_rf.score(features_test, labels_test))\n",
    "test_classifier(pipe_rf, my_dataset, my_features_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 没有加新的特征的得分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(memory=None,\n",
      "     steps=[('sc', StandardScaler(copy=True, with_mean=True, with_std=True)), ('pca', PCA(copy=True, iterated_power='auto', n_components=2, random_state=None,\n",
      "  svd_solver='auto', tol=0.0, whiten=False)), ('clf', LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False))])\n",
      "\tAccuracy: 0.86187\tPrecision: 0.45300\tRecall: 0.17350\tF1: 0.25090\tF2: 0.19792\n",
      "\tTotal predictions: 15000\tTrue positives:  347\tFalse positives:  419\tFalse negatives: 1653\tTrue negatives: 12581\n",
      "\n",
      "Pipeline(memory=None,\n",
      "     steps=[('sc', StandardScaler(copy=True, with_mean=True, with_std=True)), ('pca', PCA(copy=True, iterated_power='auto', n_components=2, random_state=None,\n",
      "  svd_solver='auto', tol=0.0, whiten=False)), ('clf', GaussianNB(priors=None))])\n",
      "\tAccuracy: 0.85927\tPrecision: 0.46094\tRecall: 0.32750\tF1: 0.38293\tF2: 0.34763\n",
      "\tTotal predictions: 15000\tTrue positives:  655\tFalse positives:  766\tFalse negatives: 1345\tTrue negatives: 12234\n",
      "\n",
      "Pipeline(memory=None,\n",
      "     steps=[('sc', StandardScaler(copy=True, with_mean=True, with_std=True)), ('pca', PCA(copy=True, iterated_power='auto', n_components=2, random_state=None,\n",
      "  svd_solver='auto', tol=0.0, whiten=False)), ('clf', DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_...      min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best'))])\n",
      "\tAccuracy: 0.78987\tPrecision: 0.19329\tRecall: 0.18150\tF1: 0.18721\tF2: 0.18374\n",
      "\tTotal predictions: 15000\tTrue positives:  363\tFalse positives: 1515\tFalse negatives: 1637\tTrue negatives: 11485\n",
      "\n",
      "Pipeline(memory=None,\n",
      "     steps=[('pca', PCA(copy=True, iterated_power='auto', n_components=2, random_state=None,\n",
      "  svd_solver='auto', tol=0.0, whiten=False)), ('clf', RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            ...n_jobs=1,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False))])\n",
      "\tAccuracy: 0.83680\tPrecision: 0.25166\tRecall: 0.11350\tF1: 0.15644\tF2: 0.12750\n",
      "\tTotal predictions: 15000\tTrue positives:  227\tFalse positives:  675\tFalse negatives: 1773\tTrue negatives: 12325\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#LogisticRegression\n",
    "pipe_lr = Pipeline([('sc', StandardScaler()),\n",
    "                    ('pca', PCA(n_components=2)),\n",
    "                    ('clf', LogisticRegression())])   #random_state=1\n",
    "pipe_lr.fit(features_train, labels_train)\n",
    "test_classifier(pipe_lr, my_dataset, features_list)\n",
    "\n",
    "#GaussianNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "pipe_nb = Pipeline([('sc', StandardScaler()),\n",
    "                    ('pca', PCA(n_components=2)),\n",
    "                    ('clf', GaussianNB()) ])\n",
    "pipe_nb.fit(features_train, labels_train)\n",
    "test_classifier(pipe_nb, my_dataset, features_list)\n",
    "\n",
    "#DecisionTreeClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "tree = DecisionTreeClassifier()\n",
    "pipe_tr = Pipeline([ ('sc', StandardScaler()),\n",
    "                    ('pca', PCA(n_components=2)),\n",
    "                    ('clf', tree)])\n",
    "pipe_tr.fit(features_train, labels_train)\n",
    "test_classifier(pipe_tr, my_dataset, features_list)\n",
    "\n",
    "#RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = RandomForestClassifier()   #max_depth=2, random_state=0\n",
    "pipe_rf = Pipeline([('pca', PCA(n_components=2)),\n",
    "                    ('clf', clf)])\n",
    "pipe_rf.fit(features_train, labels_train)\n",
    "test_classifier(pipe_rf, my_dataset, features_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "||precision|recall|\n",
    "|---|---|---|\n",
    "|LogisticRegression|0.58347|0.17300|\n",
    "|没加新特征|0.45300|0.17350|\n",
    "|GaussianNB|0.44684|0.33200|\n",
    "|没加新特征|0.46094|0.18400|\n",
    "|DecisionTreeClassifier|0.26048|0.22050|\n",
    "|没加新特征|0.19207|0.18400|\n",
    "|RandomForestClassifier|0.25532|0.10800|\n",
    "|没加新特征|0.27126|0.11800|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "把seleckbest加入算法中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(memory=None,\n",
      "     steps=[('univ_select', SelectKBest(k=7, score_func=<function f_classif at 0x00000000080D5198>)), ('sc', StandardScaler(copy=True, with_mean=True, with_std=True)), ('pca', PCA(copy=True, iterated_power='auto', n_components=2, random_state=None,\n",
      "  svd_solver='auto', tol=0.0, whiten=False)), ('clf', Lo...ty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False))])\n",
      "\tAccuracy: 0.86320\tPrecision: 0.46049\tRecall: 0.15150\tF1: 0.22799\tF2: 0.17498\n",
      "\tTotal predictions: 15000\tTrue positives:  303\tFalse positives:  355\tFalse negatives: 1697\tTrue negatives: 12645\n",
      "\n",
      "Pipeline(memory=None,\n",
      "     steps=[('univ_select', SelectKBest(k=7, score_func=<function f_classif at 0x00000000080D5198>)), ('sc', StandardScaler(copy=True, with_mean=True, with_std=True)), ('pca', PCA(copy=True, iterated_power='auto', n_components=2, random_state=None,\n",
      "  svd_solver='auto', tol=0.0, whiten=False)), ('clf', GaussianNB(priors=None))])\n",
      "\tAccuracy: 0.85160\tPrecision: 0.41281\tRecall: 0.26750\tF1: 0.32464\tF2: 0.28776\n",
      "\tTotal predictions: 15000\tTrue positives:  535\tFalse positives:  761\tFalse negatives: 1465\tTrue negatives: 12239\n",
      "\n",
      "Pipeline(memory=None,\n",
      "     steps=[('univ_select', SelectKBest(k=7, score_func=<function f_classif at 0x00000000080D5198>)), ('sc', StandardScaler(copy=True, with_mean=True, with_std=True)), ('pca', PCA(copy=True, iterated_power='auto', n_components=2, random_state=None,\n",
      "  svd_solver='auto', tol=0.0, whiten=False)), ('clf', De...      min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best'))])\n",
      "\tAccuracy: 0.82127\tPrecision: 0.31898\tRecall: 0.30000\tF1: 0.30920\tF2: 0.30361\n",
      "\tTotal predictions: 15000\tTrue positives:  600\tFalse positives: 1281\tFalse negatives: 1400\tTrue negatives: 11719\n",
      "\n",
      "Pipeline(memory=None,\n",
      "     steps=[('univ_select', SelectKBest(k=7, score_func=<function f_classif at 0x00000000080D5198>)), ('pca', PCA(copy=True, iterated_power='auto', n_components=2, random_state=None,\n",
      "  svd_solver='auto', tol=0.0, whiten=False)), ('clf', RandomForestClassifier(bootstrap=True, class_weight=None, criterion=...n_jobs=1,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False))])\n",
      "\tAccuracy: 0.84560\tPrecision: 0.32826\tRecall: 0.15100\tF1: 0.20685\tF2: 0.16928\n",
      "\tTotal predictions: 15000\tTrue positives:  302\tFalse positives:  618\tFalse negatives: 1698\tTrue negatives: 12382\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Task 4: Try a varity of classifiers\n",
    "### Please name your classifier clf for easy export below.\n",
    "### Note that if you want to do PCA or other multi-stage operations,\n",
    "### you'll need to use Pipelines. For more info:\n",
    "### http://scikit-learn.org/stable/modules/pipeline.html\n",
    "\n",
    "# Provided to give you a starting point. Try a variety of classifiers.\n",
    "\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import KernelPCA\n",
    "\n",
    "# Provided to give you a starting point. Try a variety of classifiers.\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "#LogisticRegression\n",
    "pipe_lr = Pipeline([('univ_select',SelectKBest(k=7)),\n",
    "                    ('sc', StandardScaler()),\n",
    "                    ('pca', PCA(n_components=2)),\n",
    "                    ('clf', LogisticRegression())])   #random_state=1\n",
    "pipe_lr.fit(features_train, labels_train)\n",
    "test_classifier(pipe_lr, my_dataset, my_features_list)\n",
    "\n",
    "#GaussianNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "pipe_nb = Pipeline([('univ_select',SelectKBest(k=7)),\n",
    "                    ('sc', StandardScaler()),\n",
    "                    ('pca', PCA(n_components=2)),\n",
    "                    ('clf', GaussianNB())])\n",
    "pipe_nb.fit(features_train, labels_train)\n",
    "test_classifier(pipe_nb, my_dataset, my_features_list)\n",
    "\n",
    "#DecisionTreeClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "tree = DecisionTreeClassifier()\n",
    "pipe_tr = Pipeline([('univ_select',SelectKBest(k=7)),\n",
    "                    ('sc', StandardScaler()),\n",
    "                    ('pca', PCA(n_components=2)),\n",
    "                    ('clf', tree)])\n",
    "pipe_tr.fit(features_train, labels_train)\n",
    "test_classifier(pipe_tr, my_dataset, my_features_list)\n",
    "\n",
    "#RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = RandomForestClassifier()   #max_depth=2, random_state=0\n",
    "pipe_rf = Pipeline([('univ_select',SelectKBest(k=7)),\n",
    "                    ('pca', PCA(n_components=2)),\n",
    "                    ('clf', clf)])\n",
    "pipe_rf.fit(features_train, labels_train)\n",
    "test_classifier(pipe_rf, my_dataset, my_features_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Algorithm  | Accuracy |Precision |Recall|F1 |  F2|\n",
    "|-----         |------|-----|----|-----|-----|\n",
    "|LogisticRegression    |0.87327   |0.58347   | 0.17300|0.26687  |0.20133|\n",
    "|增加selectkbest|0.86320|0.46049|0.15150|0.22799|0.17498|\n",
    "|GaussianNB      |0.85613    | 0.44684  |0.33200 | 0.38095|0.34999|\n",
    "|增加selectkbest|0.85160| 0.41281|0.26750|0.32464|0.28776|\n",
    "|DecisionTreeClassifier|0.81153  |0.25691  |0.21850|0.23615  |0.22523|\n",
    "|增加selectkbest|0.82140|0.32028|0.30250|0.31113|0.30590|\n",
    "|RandomForestClassifier|0.83827|0.25175|0.10800| 0.15115|0.12192|\n",
    "|增加selectkbest|0.84873|0.34871|0.15500|0.21461|0.17437|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4种算法中加入selectkbest后，其中LogisticRegression和GaussianNB效果没有不加好，  \n",
    "而DecisionTreeClassifier和RandomForestClassifier增加之后，各部分得分都明显提高。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "缩放特征，选择SVC算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got a divide by zero when trying out: SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n",
      "Precision or recall may be undefined due to a lack of true positive predicitons.\n"
     ]
    }
   ],
   "source": [
    " from sklearn import svm\n",
    "clf = svm.SVC()   \n",
    "clf.fit(features_train, labels_train)\n",
    "test_classifier(clf, my_dataset, my_features_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVC效果不是很好"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NearestCentroid(metric='euclidean', shrink_threshold=None)\n",
      "\tAccuracy: 0.83680\tPrecision: 0.36035\tRecall: 0.28900\tF1: 0.32075\tF2: 0.30092\n",
      "\tTotal predictions: 15000\tTrue positives:  578\tFalse positives: 1026\tFalse negatives: 1422\tTrue negatives: 11974\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# KNN算法\n",
    "from sklearn.neighbors.nearest_centroid import NearestCentroid\n",
    "clf = NearestCentroid()\n",
    "clf.fit(features_train, labels_train)\n",
    "test_classifier(clf, my_dataset, my_features_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(memory=None,\n",
      "     steps=[('univ_select', SelectKBest(k=7, score_func=<function f_classif at 0x00000000080D5198>)), ('sc', StandardScaler(copy=True, with_mean=True, with_std=True)), ('pca', PCA(copy=True, iterated_power='auto', n_components=2, random_state=None,\n",
      "  svd_solver='auto', tol=0.0, whiten=False)), ('clf', NearestCentroid(metric='euclidean', shrink_threshold=None))])\n",
      "\tAccuracy: 0.83247\tPrecision: 0.38544\tRecall: 0.43150\tF1: 0.40717\tF2: 0.42143\n",
      "\tTotal predictions: 15000\tTrue positives:  863\tFalse positives: 1376\tFalse negatives: 1137\tTrue negatives: 11624\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = NearestCentroid()\n",
    "pipe_knn = Pipeline([('univ_select',SelectKBest(k=7)),\n",
    "                    ('sc', StandardScaler()),\n",
    "                    ('pca', PCA(n_components=2)),\n",
    "                    ('clf', NearestCentroid())])\n",
    "pipe_knn.fit(features_train, labels_train)\n",
    "test_classifier(pipe_knn, my_dataset, my_features_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "调参之后KNN算法各项指标提高了很多"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 调参"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=10, error_score='raise',\n",
       "       estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid=[{'penalty': ('l1', 'l2'), 'C': [0.1, 1.0, 10, 100]}],\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring='f1', verbose=0)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LogisticRegression\n",
    "param_grid =[{'penalty': ('l1', 'l2'),\n",
    "               'C': [0.1, 1.0, 10, 100]}]\n",
    "\n",
    "lr = GridSearchCV( LogisticRegression(),\n",
    "                  param_grid = param_grid, scoring = 'f1', cv = 10)\n",
    "lr.fit(features_train, labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "lr_best = lr.best_estimator_ #LogisticRegression\n",
    "print lr_best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LogisticRegression 调参结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n",
      "\tAccuracy: 0.86220\tPrecision: 0.47079\tRecall: 0.27000\tF1: 0.34318\tF2: 0.29518\n",
      "\tTotal predictions: 15000\tTrue positives:  540\tFalse positives:  607\tFalse negatives: 1460\tTrue negatives: 12393\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_classifier(lr_best, my_dataset, my_features_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=10, error_score='raise',\n",
       "       estimator=RandomForestClassifier(bootstrap=True, class_weight='balanced',\n",
       "            criterion='gini', max_depth=None, max_features='auto',\n",
       "            max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "            min_impurity_split=None, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            n_estimators=10, n_jobs=1, oob_score=False, random_state=42,\n",
       "            verbose=0, warm_start=False),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid=[{'n_estimators': [5, 10, 50], 'min_samples_split': [2, 5, 10, 20, 40, 70], 'criterion': ('gini', 'entropy')}],\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring='f1', verbose=0)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#RandomForestClassifier\n",
    "param_grid =[{ 'n_estimators': [5, 10, 50],\n",
    "               'criterion': ('gini', 'entropy'), \n",
    "               'min_samples_split': [2, 5, 10, 20, 40, 70]}]\n",
    "\n",
    "rf = GridSearchCV( RandomForestClassifier(class_weight='balanced', random_state=42),\n",
    "                  param_grid = param_grid, scoring = 'f1', cv = 10)\n",
    "rf.fit(features_train, labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier(bootstrap=True, class_weight='balanced',\n",
      "            criterion='gini', max_depth=None, max_features='auto',\n",
      "            max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "            min_impurity_split=None, min_samples_leaf=1,\n",
      "            min_samples_split=20, min_weight_fraction_leaf=0.0,\n",
      "            n_estimators=10, n_jobs=1, oob_score=False, random_state=42,\n",
      "            verbose=0, warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "rf_best = rf.best_estimator_ # RandomForestClassifier\n",
    "print rf_best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RandomForestClassifier 调参结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier(bootstrap=True, class_weight='balanced',\n",
      "            criterion='gini', max_depth=None, max_features='auto',\n",
      "            max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "            min_impurity_split=None, min_samples_leaf=1,\n",
      "            min_samples_split=20, min_weight_fraction_leaf=0.0,\n",
      "            n_estimators=10, n_jobs=1, oob_score=False, random_state=42,\n",
      "            verbose=0, warm_start=False)\n",
      "\tAccuracy: 0.80680\tPrecision: 0.33209\tRecall: 0.44400\tF1: 0.37997\tF2: 0.41596\n",
      "\tTotal predictions: 15000\tTrue positives:  888\tFalse positives: 1786\tFalse negatives: 1112\tTrue negatives: 11214\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_classifier(rf_best, my_dataset, my_features_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GridSearchCV(cv=10, error_score='raise',\n",
      "       estimator=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best'),\n",
      "       fit_params=None, iid=True, n_jobs=1,\n",
      "       param_grid=[{'min_samples_split': [2, 3, 4, 5, 6, 7, 8, 9, 10], 'splitter': ('best', 'random'), 'max_depth': [None, 10, 30, 50, 70, 100]}],\n",
      "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
      "       scoring='f1', verbose=0)\n",
      "\tAccuracy: 0.82233\tPrecision: 0.31456\tRecall: 0.28200\tF1: 0.29739\tF2: 0.28796\n",
      "\tTotal predictions: 15000\tTrue positives:  564\tFalse positives: 1229\tFalse negatives: 1436\tTrue negatives: 11771\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#DecisionTreeClassifier\n",
    "param_grid =[{ 'splitter': ('best', 'random'),\n",
    "               'min_samples_split': [2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    "               'max_depth': [None, 10, 30, 50, 70, 100] }]\n",
    "\n",
    "tr = GridSearchCV( DecisionTreeClassifier(),\n",
    "                  param_grid = param_grid, scoring = 'f1', cv = 10)\n",
    "tr.fit(features_train, labels_train)\n",
    "test_classifier(tr, my_dataset, my_features_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=10, error_score='raise',\n",
       "       estimator=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "            splitter='best'),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid=[{'min_samples_split': [2, 3, 4, 5, 6, 7, 8, 9, 10], 'splitter': ('best', 'random'), 'max_depth': [None, 10, 30, 50, 70, 100]}],\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring='f1', verbose=0)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#DecisionTreeClassifier\n",
    "param_grid =[{ 'splitter': ('best', 'random'),\n",
    "               'min_samples_split': [2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    "               'max_depth': [None, 10, 30, 50, 70, 100] }]\n",
    "\n",
    "tr = GridSearchCV( DecisionTreeClassifier(),\n",
    "                  param_grid = param_grid, scoring = 'f1', cv = 10)\n",
    "tr.fit(features_train, labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='random')\n"
     ]
    }
   ],
   "source": [
    "tr_best = tr.best_estimator_ # DecisionTreeClassifier\n",
    "print tr_best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DecisionTreeClassifier调参结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='random')\n",
      "\tAccuracy: 0.80600\tPrecision: 0.30200\tRecall: 0.34700\tF1: 0.32294\tF2: 0.33696\n",
      "\tTotal predictions: 15000\tTrue positives:  694\tFalse positives: 1604\tFalse negatives: 1306\tTrue negatives: 11396\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_classifier(tr_best, my_dataset, my_features_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example starting point. Try investigating other evaluation techniques!\n",
    "from sklearn.cross_validation import train_test_split\n",
    "features_train, features_test, labels_train, labels_test = \\\n",
    "    train_test_split(features, labels, test_size=0.3, random_state=42)\n",
    "\n",
    "### Task 6: Dump your classifier, dataset, and features_list so anyone can\n",
    "### check your results. You do not need to change anything below, but make sure\n",
    "### that the version of poi_id.py that you submit can be run on its own and\n",
    "### generates the necessary .pkl files for validating your results.\n",
    "\n",
    "dump_classifier_and_data(clf, my_dataset, features_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
